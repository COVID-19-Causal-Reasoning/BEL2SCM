{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from functools import partial\n",
    "import torch\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "import pyro\n",
    "import pyro.distributions as dist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for Continuous Integration testing\n",
    "smoke_test = ('CI' in os.environ)\n",
    "# assert pyro.__version__.startswith('1.3.0')\n",
    "pyro.enable_validation(True)\n",
    "pyro.set_rng_seed(1)\n",
    "pyro.enable_validation(True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    " mapk_data=pd.read_csv(\"data\\\\mapk100_rm1.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>Raf</th>\n",
       "      <th>Mek</th>\n",
       "      <th>Erk</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2</td>\n",
       "      <td>47</td>\n",
       "      <td>64</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>3</td>\n",
       "      <td>47</td>\n",
       "      <td>62</td>\n",
       "      <td>84</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4</td>\n",
       "      <td>50</td>\n",
       "      <td>66</td>\n",
       "      <td>89</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>5</td>\n",
       "      <td>44</td>\n",
       "      <td>66</td>\n",
       "      <td>91</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>6</td>\n",
       "      <td>42</td>\n",
       "      <td>55</td>\n",
       "      <td>82</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  Raf  Mek  Erk\n",
       "0           2   47   64   82\n",
       "1           3   47   62   84\n",
       "2           4   50   66   89\n",
       "3           5   44   66   91\n",
       "4           6   42   55   82"
      ]
     },
     "execution_count": 57,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mapk_data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [],
   "source": [
    "mapk_data=mapk_data.drop(columns='Unnamed: 0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from pyro.nn import PyroModule\n",
    "\n",
    "assert issubclass(PyroModule[nn.Linear], nn.Linear)\n",
    "assert issubclass(PyroModule[nn.Linear], PyroModule)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "metadata": {},
   "outputs": [],
   "source": [
    "data=data = torch.tensor(mapk_data[[\"Raf\", \"Mek\", \"Erk\"]].values,\n",
    "                        dtype=torch.float)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([82., 84., 89., 91., 82.])"
      ]
     },
     "execution_count": 64,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "x_data, y_data = data[:,1:2], data[:,2]\n",
    "\n",
    "x_squared_data = torch.cat((x_data, x_data*x_data, 1/x_data), 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0050] loss: 187905.7344\n",
      "[iteration 0100] loss: 18869.7480\n",
      "[iteration 0150] loss: 13182.9131\n",
      "[iteration 0200] loss: 13151.2852\n",
      "[iteration 0250] loss: 13128.2686\n",
      "[iteration 0300] loss: 13102.7725\n",
      "[iteration 0350] loss: 13074.7549\n",
      "[iteration 0400] loss: 13044.3252\n",
      "[iteration 0450] loss: 13011.5859\n",
      "[iteration 0500] loss: 12976.6055\n",
      "[iteration 0550] loss: 12939.4463\n",
      "[iteration 0600] loss: 12900.1553\n",
      "[iteration 0650] loss: 12858.7754\n",
      "[iteration 0700] loss: 12815.3340\n",
      "[iteration 0750] loss: 12769.8623\n",
      "[iteration 0800] loss: 12722.3838\n",
      "[iteration 0850] loss: 12672.9131\n",
      "[iteration 0900] loss: 12621.4766\n",
      "[iteration 0950] loss: 12568.0801\n",
      "[iteration 1000] loss: 12512.7402\n",
      "[iteration 1050] loss: 12455.4688\n",
      "[iteration 1100] loss: 12396.2725\n",
      "[iteration 1150] loss: 12335.1631\n",
      "[iteration 1200] loss: 12272.1455\n",
      "[iteration 1250] loss: 12207.2236\n",
      "[iteration 1300] loss: 12140.4150\n",
      "[iteration 1350] loss: 12071.7148\n",
      "[iteration 1400] loss: 12001.1367\n",
      "[iteration 1450] loss: 11928.6777\n",
      "[iteration 1500] loss: 11854.3574\n",
      "[iteration 1550] loss: 11778.1748\n",
      "[iteration 1600] loss: 11700.1367\n",
      "[iteration 1650] loss: 11620.2500\n",
      "[iteration 1700] loss: 11538.5303\n",
      "[iteration 1750] loss: 11454.9775\n",
      "[iteration 1800] loss: 11369.6016\n",
      "[iteration 1850] loss: 11282.4268\n",
      "[iteration 1900] loss: 11193.4453\n",
      "[iteration 1950] loss: 11102.6846\n",
      "[iteration 2000] loss: 11010.1484\n",
      "[iteration 2050] loss: 10915.8545\n",
      "[iteration 2100] loss: 10819.8184\n",
      "[iteration 2150] loss: 10722.0596\n",
      "[iteration 2200] loss: 10622.5967\n",
      "[iteration 2250] loss: 10521.4512\n",
      "[iteration 2300] loss: 10418.6465\n",
      "[iteration 2350] loss: 10314.2021\n",
      "[iteration 2400] loss: 10208.1455\n",
      "[iteration 2450] loss: 10100.5020\n",
      "[iteration 2500] loss: 9991.3076\n",
      "[iteration 2550] loss: 9880.5840\n",
      "[iteration 2600] loss: 9768.3770\n",
      "[iteration 2650] loss: 9654.7109\n",
      "[iteration 2700] loss: 9539.6309\n",
      "[iteration 2750] loss: 9423.1719\n",
      "[iteration 2800] loss: 9305.3799\n",
      "[iteration 2850] loss: 9186.2959\n",
      "[iteration 2900] loss: 9065.9746\n",
      "[iteration 2950] loss: 8944.4590\n",
      "[iteration 3000] loss: 8821.7988\n",
      "[iteration 3050] loss: 8698.0576\n",
      "[iteration 3100] loss: 8573.2812\n",
      "[iteration 3150] loss: 8447.5371\n",
      "[iteration 3200] loss: 8320.8867\n",
      "[iteration 3250] loss: 8193.3896\n",
      "[iteration 3300] loss: 8065.1216\n",
      "[iteration 3350] loss: 7936.1421\n",
      "[iteration 3400] loss: 7806.5327\n",
      "[iteration 3450] loss: 7676.3647\n",
      "[iteration 3500] loss: 7545.7129\n",
      "[iteration 3550] loss: 7414.6592\n",
      "[iteration 3600] loss: 7283.2866\n",
      "[iteration 3650] loss: 7151.6729\n",
      "[iteration 3700] loss: 7019.9146\n",
      "[iteration 3750] loss: 6888.0908\n",
      "[iteration 3800] loss: 6756.2988\n",
      "[iteration 3850] loss: 6624.6294\n",
      "[iteration 3900] loss: 6493.1694\n",
      "[iteration 3950] loss: 6362.0229\n",
      "[iteration 4000] loss: 6231.2861\n",
      "[iteration 4050] loss: 6101.0522\n",
      "[iteration 4100] loss: 5971.4268\n",
      "[iteration 4150] loss: 5842.5063\n",
      "[iteration 4200] loss: 5714.3931\n",
      "[iteration 4250] loss: 5587.1875\n",
      "[iteration 4300] loss: 5460.9961\n",
      "[iteration 4350] loss: 5335.9136\n",
      "[iteration 4400] loss: 5212.0405\n",
      "[iteration 4450] loss: 5089.4795\n",
      "[iteration 4500] loss: 4968.3340\n",
      "[iteration 4550] loss: 4848.6982\n",
      "[iteration 4600] loss: 4730.6704\n",
      "[iteration 4650] loss: 4614.3501\n",
      "[iteration 4700] loss: 4499.8281\n",
      "[iteration 4750] loss: 4387.1953\n",
      "[iteration 4800] loss: 4276.5405\n",
      "[iteration 4850] loss: 4167.9502\n",
      "[iteration 4900] loss: 4061.5073\n",
      "[iteration 4950] loss: 3957.2883\n",
      "[iteration 5000] loss: 3855.3733\n",
      "[iteration 5050] loss: 3755.8289\n",
      "[iteration 5100] loss: 3658.7278\n",
      "[iteration 5150] loss: 3564.1230\n",
      "[iteration 5200] loss: 3472.0771\n",
      "[iteration 5250] loss: 3382.6399\n",
      "[iteration 5300] loss: 3295.8562\n",
      "[iteration 5350] loss: 3211.7671\n",
      "[iteration 5400] loss: 3130.4104\n",
      "[iteration 5450] loss: 3051.8123\n",
      "[iteration 5500] loss: 2975.9944\n",
      "[iteration 5550] loss: 2902.9756\n",
      "[iteration 5600] loss: 2832.7603\n",
      "[iteration 5650] loss: 2765.3540\n",
      "[iteration 5700] loss: 2700.7561\n",
      "[iteration 5750] loss: 2638.9529\n",
      "[iteration 5800] loss: 2579.9304\n",
      "[iteration 5850] loss: 2523.6655\n",
      "[iteration 5900] loss: 2470.1318\n",
      "[iteration 5950] loss: 2419.2893\n",
      "[iteration 6000] loss: 2371.0989\n",
      "[iteration 6050] loss: 2325.5154\n",
      "[iteration 6100] loss: 2282.4844\n",
      "[iteration 6150] loss: 2241.9487\n",
      "[iteration 6200] loss: 2203.8477\n",
      "[iteration 6250] loss: 2168.1162\n",
      "[iteration 6300] loss: 2134.6819\n",
      "[iteration 6350] loss: 2103.4695\n",
      "[iteration 6400] loss: 2074.4016\n",
      "[iteration 6450] loss: 2047.3956\n",
      "[iteration 6500] loss: 2022.3719\n",
      "[iteration 6550] loss: 1999.2432\n",
      "[iteration 6600] loss: 1977.9214\n",
      "[iteration 6650] loss: 1958.3206\n",
      "[iteration 6700] loss: 1940.3522\n",
      "[iteration 6750] loss: 1923.9259\n",
      "[iteration 6800] loss: 1908.9561\n",
      "[iteration 6850] loss: 1895.3510\n",
      "[iteration 6900] loss: 1883.0255\n",
      "[iteration 6950] loss: 1871.8960\n",
      "[iteration 7000] loss: 1861.8783\n",
      "[iteration 7050] loss: 1852.8915\n",
      "[iteration 7100] loss: 1844.8566\n",
      "[iteration 7150] loss: 1837.6970\n",
      "[iteration 7200] loss: 1831.3425\n",
      "[iteration 7250] loss: 1825.7214\n",
      "[iteration 7300] loss: 1820.7688\n",
      "[iteration 7350] loss: 1816.4218\n",
      "[iteration 7400] loss: 1812.6208\n",
      "[iteration 7450] loss: 1809.3114\n",
      "[iteration 7500] loss: 1806.4414\n",
      "[iteration 7550] loss: 1803.9630\n",
      "[iteration 7600] loss: 1801.8328\n",
      "[iteration 7650] loss: 1800.0089\n",
      "[iteration 7700] loss: 1798.4541\n",
      "[iteration 7750] loss: 1797.1362\n",
      "[iteration 7800] loss: 1796.0237\n",
      "[iteration 7850] loss: 1795.0870\n",
      "[iteration 7900] loss: 1794.3043\n",
      "[iteration 7950] loss: 1793.6533\n",
      "[iteration 8000] loss: 1793.1146\n",
      "[iteration 8050] loss: 1792.6680\n",
      "[iteration 8100] loss: 1792.3029\n",
      "[iteration 8150] loss: 1792.0023\n",
      "[iteration 8200] loss: 1791.7605\n",
      "[iteration 8250] loss: 1791.5621\n",
      "[iteration 8300] loss: 1791.4015\n",
      "[iteration 8350] loss: 1791.2732\n",
      "[iteration 8400] loss: 1791.1680\n",
      "[iteration 8450] loss: 1791.0826\n",
      "[iteration 8500] loss: 1791.0138\n",
      "[iteration 8550] loss: 1790.9583\n",
      "[iteration 8600] loss: 1790.9110\n",
      "[iteration 8650] loss: 1790.8718\n",
      "[iteration 8700] loss: 1790.8392\n",
      "[iteration 8750] loss: 1790.8101\n",
      "[iteration 8800] loss: 1790.7832\n",
      "[iteration 8850] loss: 1790.7609\n",
      "[iteration 8900] loss: 1790.7378\n",
      "[iteration 8950] loss: 1790.7186\n",
      "[iteration 9000] loss: 1790.6989\n",
      "[iteration 9050] loss: 1790.6792\n",
      "[iteration 9100] loss: 1790.6608\n",
      "[iteration 9150] loss: 1790.6399\n",
      "[iteration 9200] loss: 1790.6208\n",
      "[iteration 9250] loss: 1790.6016\n",
      "[iteration 9300] loss: 1790.5815\n",
      "[iteration 9350] loss: 1790.5614\n",
      "[iteration 9400] loss: 1790.5399\n",
      "[iteration 9450] loss: 1790.5189\n",
      "[iteration 9500] loss: 1790.4968\n",
      "[iteration 9550] loss: 1790.4746\n",
      "[iteration 9600] loss: 1790.4519\n",
      "[iteration 9650] loss: 1790.4282\n",
      "[iteration 9700] loss: 1790.4044\n",
      "[iteration 9750] loss: 1790.3794\n",
      "[iteration 9800] loss: 1790.3549\n",
      "[iteration 9850] loss: 1936.1831\n",
      "[iteration 9900] loss: 1792.2804\n",
      "[iteration 9950] loss: 1790.3033\n",
      "[iteration 10000] loss: 1790.2599\n",
      "Learned parameters:\n",
      "weight [[ 2.4478395  -0.01869777  7.3390284 ]]\n",
      "bias [4.2769265]\n"
     ]
    }
   ],
   "source": [
    "# Regression model\n",
    "linear_reg_model = PyroModule[nn.Linear](3,1)\n",
    "\n",
    "# Define loss and optimize\n",
    "loss_fn = torch.nn.MSELoss(reduction='sum')\n",
    "optim = torch.optim.Adam(linear_reg_model.parameters(), lr=0.01)\n",
    "num_iterations = 10000 if not smoke_test else 2\n",
    "\n",
    "def train():\n",
    "    # run the model forward on the data\n",
    "    y_pred = linear_reg_model(x_squared_data).squeeze(-1)\n",
    "    # calculate the mse loss\n",
    "    loss = loss_fn(y_pred, y_data)\n",
    "    # initialize gradients to zero\n",
    "    optim.zero_grad()\n",
    "    # backpropagate\n",
    "    loss.backward()\n",
    "    # take a gradient step\n",
    "    optim.step()\n",
    "    return loss\n",
    "\n",
    "for j in range(num_iterations):\n",
    "    loss = train()\n",
    "    if (j + 1) % 50 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss.item()))\n",
    "\n",
    "# Inspect learned parameters\n",
    "print(\"Learned parameters:\")\n",
    "for name, param in linear_reg_model.named_parameters():\n",
    "    print(name, param.data.numpy())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "   Raf  Mek  Erk\n",
      "0   47   64   82\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "tensor([61.7021], grad_fn=<AddBackward0>)"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "print(mapk_data[:1])\n",
    "linear_reg_model(torch.tensor([46.8, 46.8*46.8]))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### Bayesian Regression with Pyro’s Stochastic Variational Inference (SVI)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.nn import PyroSample\n",
    "\n",
    "\n",
    "class BayesianRegression(PyroModule):\n",
    "    def __init__(self, in_features, out_features):\n",
    "        super().__init__()\n",
    "        self.linear = PyroModule[nn.Linear](in_features, out_features)\n",
    "        self.linear.weight = PyroSample(dist.Normal(0., 1.).expand([out_features, in_features]).to_event(2))\n",
    "        self.linear.bias = PyroSample(dist.Normal(0., 10.).expand([out_features]).to_event(1))\n",
    "\n",
    "    def forward(self, x, y=None):\n",
    "        sigma = pyro.sample(\"sigma\", dist.Uniform(0., 10.))\n",
    "        mean = self.linear(x).squeeze(-1)\n",
    "        with pyro.plate(\"data\", x.shape[0]):\n",
    "            obs = pyro.sample(\"obs\", dist.Normal(mean, sigma), obs=y)\n",
    "        return mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer.autoguide import AutoDiagonalNormal\n",
    "\n",
    "model = BayesianRegression(1, 1)\n",
    "guide = AutoDiagonalNormal(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyro.infer import SVI, Trace_ELBO\n",
    "\n",
    "\n",
    "adam = pyro.optim.Adam({\"lr\": 0.03})\n",
    "svi = SVI(model, guide, adam, loss=Trace_ELBO())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[iteration 0001] loss: 190.6467\n",
      "[iteration 0101] loss: 3.4870\n",
      "[iteration 0201] loss: 3.2757\n",
      "[iteration 0301] loss: 3.7104\n",
      "[iteration 0401] loss: 3.2371\n",
      "[iteration 0501] loss: 3.2677\n",
      "[iteration 0601] loss: 3.2544\n",
      "[iteration 0701] loss: 3.2334\n",
      "[iteration 0801] loss: 3.3157\n",
      "[iteration 0901] loss: 3.2346\n",
      "[iteration 1001] loss: 3.2213\n",
      "[iteration 1101] loss: 3.2331\n",
      "[iteration 1201] loss: 3.3399\n",
      "[iteration 1301] loss: 3.2730\n",
      "[iteration 1401] loss: 3.2356\n"
     ]
    }
   ],
   "source": [
    "pyro.clear_param_store()\n",
    "for j in range(num_iterations):\n",
    "    # calculate the loss and take a gradient step\n",
    "    loss = svi.step(x_data, y_data)\n",
    "    if j % 100 == 0:\n",
    "        print(\"[iteration %04d] loss: %.4f\" % (j + 1, loss / len(data)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "AutoDiagonalNormal.loc Parameter containing:\n",
      "tensor([0.4395, 1.2235, 5.9731])\n",
      "AutoDiagonalNormal.scale tensor([0.1073, 0.0233, 0.7513])\n"
     ]
    }
   ],
   "source": [
    "guide.requires_grad_(False)\n",
    "\n",
    "for name, value in pyro.get_param_store().items():\n",
    "    print(name, pyro.param(name))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'sigma': [tensor(5.9076), tensor(6.0814), tensor(6.2524)],\n",
       " 'linear.weight': [tensor([[1.2078]]), tensor([[1.2235]]), tensor([[1.2392]])],\n",
       " 'linear.bias': [tensor([5.4663]), tensor([5.9731]), tensor([6.4798])]}"
      ]
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "guide.quantiles([0.25, 0.5, 0.75])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
